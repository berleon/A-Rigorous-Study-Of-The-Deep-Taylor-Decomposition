{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to visualize root of DTD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=\"\"\n",
    "\n",
    "\n",
    "from typing import Union, Callable\n",
    "\n",
    "import torch \n",
    "import numpy as np\n",
    "import tqdm.auto\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from lrp_relations import dtd\n",
    "from lrp_relations.utils import to_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "mlp = dtd.MLP(5, 10, 10, 2)\n",
    "\n",
    "def weight_scale(m: nn.Module) -> nn.Module:\n",
    "    for p in m.parameters():\n",
    "        # to compensate for negative biases, we scale the weight\n",
    "        p.data[p.data > 0] = 1.4 * p.data[p.data > 0]\n",
    "    if isinstance(m, dtd.LinearReLU):\n",
    "        m.linear.bias.data = - m.linear.bias.data.abs() \n",
    "    return m\n",
    "\n",
    "mlp.apply(weight_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(11, 10)\n",
    "\n",
    "logits = mlp(x)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_summary(df: pd.DataFrame) -> None:\n",
    "    keys = [\n",
    "        \"last_layer_for_input\",\n",
    "        \"layer3_for_input\",\n",
    "        \"last_layer_for_root\",\n",
    "        \"layer3_for_root\",\n",
    "    ]\n",
    "    print('-' * 80)\n",
    "    print(df.rule.iloc[0])\n",
    "    for key in keys:\n",
    "        print(key, np.abs(np.stack(df[key])).mean())\n",
    "\n",
    "all_roots = []\n",
    "data = []\n",
    "for rule in [ 'z+', 'pinv', ]:\n",
    "    rr = dtd.RecursiveRoots(mlp, explained_class=0, rule=rule)\n",
    "\n",
    "\n",
    "    for _ in tqdm.auto.trange(200):\n",
    "        x = torch.randn(1, mlp.input_size)\n",
    "        roots = rr.run(x, start_at=mlp.layer1, end_at=mlp.layer1)\n",
    "        all_roots.extend(roots)\n",
    "        for root in roots:\n",
    "            data.append(\n",
    "                dict(\n",
    "                    rule=rule,\n",
    "                    explained_neuron=root.explained_neuron,\n",
    "                    last_layer_for_input=to_np(root.outputs_of_input[mlp.layers[-1]]),\n",
    "                    layer3_for_input=to_np(root.outputs_of_input[mlp.layers[3]]),\n",
    "                    last_layer_for_root=to_np(root.outputs_of_root[mlp.layers[-1]]),\n",
    "                    layer3_for_root=to_np(root.outputs_of_root[mlp.layers[3]]),\n",
    "                )\n",
    "            )\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.groupby('rule').apply(show_summary)\n",
    "# print('-' * 80)\n",
    "# print(rule)\n",
    "# print()\n",
    "# show_summary(df)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for root in all_roots:\n",
    "    assert root.input.shape == (1, 10), root.rule.name\n",
    "    assert root.root.shape == (1, 10), root.rule.name\n",
    "\n",
    "    root.input.requires_grad_(True)\n",
    "    root.root.requires_grad_(True)\n",
    "\n",
    "    out = mlp(root.input)\n",
    "    grad_input, = torch.autograd.grad(\n",
    "        out[:, 0],\n",
    "        root.input,\n",
    "        grad_outputs=torch.ones_like(out[:, 0]),\n",
    "    )\n",
    "\n",
    "    out = mlp(root.root)\n",
    "    grad_root, = torch.autograd.grad(\n",
    "        out[:, 0],\n",
    "        root.root,\n",
    "        grad_outputs=torch.ones_like(out[:, 0]),\n",
    "    )\n",
    "\n",
    "    grad_input, grad_root\n",
    "\n",
    "    data.append(dict(\n",
    "        rule=root.rule.name,\n",
    "        explained_neuron=root.explained_neuron,\n",
    "        grad_input=to_np(grad_input),\n",
    "        grad_root=to_np(grad_root),\n",
    "        grad_diff=(grad_input - grad_root).abs().mean().item(),\n",
    "    ))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.groupby('rule').grad_diff.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root = [r for r in all_roots if r.rule.name == 'pinv'][0]\n",
    "\n",
    "# root.outputs_of_input[mlp.layers[-1]]\n",
    "root.input, root.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('rule').grad_diff.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keys = [\n",
    "    \"last_layer_for_input\",\n",
    "    \"layer3_for_input\",\n",
    "    \"last_layer_for_root\",\n",
    "    \"layer3_for_root\",\n",
    "]\n",
    "for key in keys:\n",
    "    print(key, np.abs(np.stack(df[key])).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(roots[0].upper_layers[0].upper_layers[0].activations_for_root[mlp.layer4],\n",
    "roots[0].upper_layers[0].upper_layers[0].activations_for_input[mlp.layer4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roots = torch.cat([\n",
    "    dtd.root_point_linear(x[:1], mlp.layer1, j=j)\n",
    "    for j in range(mlp.layer1.out_features)\n",
    "])\n",
    "\n",
    "list(zip(roots, mlp.layer1(roots)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer_root = torch.linalg.pinv(mlp.layer1.linear.weight) @ mlp.layer1.linear.bias \n",
    "mlp.layer1(layer_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp.layer1(roots.mean(0, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "364ea4cd247a9314851a2e26e0d2a5938af3e08d71fd547ec79f80ae6e1a0fa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

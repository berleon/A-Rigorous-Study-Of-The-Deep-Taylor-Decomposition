{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reimplement the DTD \n",
    "\n",
    "Goal:\n",
    "\n",
    "- Compute higher order relevances\n",
    "- Follow the theory 100%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=\"\"\n",
    "\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lrp_relations import dtd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation 4, Appendix DTD\n",
    "$\n",
    "x_i - \\tilde x_i = \\frac\n",
    "    {\\sum_k (x_k w_{kj} + b_k)}\n",
    "    {\\sum_l (v_l w_{lj}}\n",
    "    v_i\n",
    "$\n",
    "\n",
    "where $ v_i = x_i 1_{w_{ij} \\ge 0} $ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "net = dtd.TwoLayerMLP(input_size=3, hidden_size=5, output_size=1)\n",
    "\n",
    "net.layer2.linear.weight.data = 10 * net.layer2.linear.weight.data.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevance_hidden(\n",
    "    net: nn.Module,\n",
    "    x: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    outputs = {}\n",
    "    handles = []\n",
    "\n",
    "    def add_hook(module: nn.Module) -> None:\n",
    "        handles.append(module.register_forward_hook(save_outputs))\n",
    "\n",
    "    def remove_hooks() -> None:\n",
    "        for handle in handles:\n",
    "            handle.remove()\n",
    "\n",
    "    def save_outputs(\n",
    "        module: nn.Module, inputs: tuple[torch.Tensor], output: torch.Tensor\n",
    "    ) -> None:\n",
    "        outputs[module] = output\n",
    "\n",
    "    net.apply(add_hook)\n",
    "\n",
    "\n",
    "    logit = net(x)\n",
    "    remove_hooks()\n",
    "\n",
    "    # outputs\n",
    "    hidden = outputs[net.layer1]\n",
    "    hidden_root = dtd.root_point(hidden, net.layer2, 0, rule=\"z+\")\n",
    "\n",
    "    output_root = net.layer2.linear(hidden_root)\n",
    "    (rel_grad,) = torch.autograd.grad(\n",
    "        output_root,\n",
    "        hidden_root,\n",
    "        grad_outputs=torch.ones_like(output_root),\n",
    "        retain_graph=True,\n",
    "    )\n",
    "    rel_hidden = rel_grad * (hidden - hidden_root)\n",
    "    return rel_hidden\n",
    "\n",
    "\n",
    "\n",
    "# x_rel_grad = torch.autograd.grad(\n",
    "#     rel_hidden,\n",
    "#     x,\n",
    "#     grad_outputs=torch.ones_like(rel_hidden),\n",
    "#     retain_graph=True,\n",
    "# )\n",
    "\n",
    "def find_root_point(\n",
    "    net: nn.Module, \n",
    "    x: torch.Tensor,\n",
    "    j: int,\n",
    "    relevance_fn: Callable[[nn.Module, torch.Tensor], torch.Tensor],\n",
    "):\n",
    "    assert x.size(0) == 1\n",
    "    x_search = x + torch.randn(1000, x.size(1))\n",
    "\n",
    "    rel_hidden_search = relevance_fn(net, x_search)\n",
    "\n",
    "\n",
    "    print((rel_hidden_search[:, j] == 0.).float().sum())\n",
    "    idx = rel_hidden_search[:, j].abs().argmin()\n",
    "\n",
    "    steps = torch.linspace(0, 1, 100).view(-1, 1)\n",
    "\n",
    "    x_line_search = x_search[idx] + steps * (x[0] - x_search[idx]).unsqueeze(0)\n",
    "\n",
    "    rel_hidden_line_search = relevance_fn(net, x_line_search)\n",
    "    root_idx = (rel_hidden_line_search[:, j] <= 1e-5).nonzero().max()\n",
    "    print(root_idx)\n",
    "    return x_line_search[root_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(3)\n",
    "x = (0.25 * torch.randn(1, 3, requires_grad=True) + 3).clamp(min=0)\n",
    "\n",
    "rel_hidden = get_relevance_hidden(net, x)\n",
    "x_root = dtd.root_point(x, net.layer1, 0)\n",
    "\n",
    "x_root_search = find_root_point(net, x, 0, get_relevance_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_root_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, x_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (LRP Relations)",
   "language": "python",
   "name": "lrp_py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

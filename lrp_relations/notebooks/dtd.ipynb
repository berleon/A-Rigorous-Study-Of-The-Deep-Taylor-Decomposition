{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reimplement the DTD \n",
    "\n",
    "Goal:\n",
    "\n",
    "- Compute higher order relevances\n",
    "- Follow the theory 100%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=\"\"\n",
    "\n",
    "\n",
    "from typing import Union, Callable\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from lrp_relations import dtd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation 4, Appendix DTD\n",
    "$\n",
    "x_i - \\tilde x_i = \\frac\n",
    "    {\\sum_k (x_k w_{kj} + b_k)}\n",
    "    {\\sum_l (v_l w_{lj}}\n",
    "    v_i\n",
    "$\n",
    "\n",
    "where $ v_i = x_i 1_{w_{ij} \\ge 0} $ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "net = dtd.TwoLayerMLP(input_size=3, hidden_size=5, output_size=1)\n",
    "\n",
    "\n",
    "data = net.layer2.linear.weight.data.clone()\n",
    "data[:, :2] = 5 * data.abs()[:, :2]\n",
    "net.layer2.linear.weight.data = data\n",
    "net.layer2.linear.weight\n",
    "net.layer2.linear.bias.data.abs_()\n",
    "net.layer1.linear.bias.data.abs_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(3)\n",
    "x = (0.25 * torch.randn(1, 3, requires_grad=True) + 3).clamp(min=0)\n",
    "\n",
    "\n",
    "rules = [\"0\", \"x\", \"z+\", \"w2\", \"gamma\"]\n",
    "mode = \"sum\"\n",
    "info = []\n",
    "for rule in rules:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Rule:\", rule)\n",
    "\n",
    "    def relevance_fn(net: nn.Module, x: torch.Tensor) -> torch.Tensor:\n",
    "        rel = dtd.get_relevance_hidden(net, x, rule=rule, gamma=1000)\n",
    "\n",
    "        if mode == \"sum\":\n",
    "            return rel.sum(dim=1, keepdim=True)\n",
    "        else:\n",
    "            return rel\n",
    "\n",
    "    with dtd.record_all_outputs(net) as x_outs:\n",
    "        logit_x = net(x)\n",
    "\n",
    "    rel_hidden = relevance_fn(net, x)\n",
    "    (grad_rel_hidden,) = torch.autograd.grad(\n",
    "        [rel_hidden[:, 0]],\n",
    "        [x],\n",
    "    )\n",
    "    hidden_root = dtd.root_point(\n",
    "        x_outs[net.layer1][0], net.layer2, 0, rule=rule, gamma=1000\n",
    "    )\n",
    "    x_root = dtd.find_input_root_point(\n",
    "        net, x, 0, relevance_fn, n_samples=20_000, plot=True\n",
    "    )\n",
    "\n",
    "    with dtd.record_all_outputs(net) as x_root_outs:\n",
    "        logit_x_root = net(x_root)\n",
    "\n",
    "    print(x_root.shape)\n",
    "    (\n",
    "        rel_hidden_for_x_root,\n",
    "        hidden_root_for_x_root,\n",
    "    ) = dtd.get_relevance_hidden_and_root(\n",
    "        net, x_root.unsqueeze(0), rule=rule, gamma=1000\n",
    "    )\n",
    "    if mode == \"sum\":\n",
    "        rel_hidden_for_x_root = rel_hidden_for_x_root.sum(dim=1, keepdim=True)\n",
    "\n",
    "    (grad_rel_hidden_for_x_root,) = torch.autograd.grad(\n",
    "        [rel_hidden_for_x_root[:, 0]],\n",
    "        [x_root],\n",
    "    )\n",
    "\n",
    "    info.append(\n",
    "        dict(\n",
    "            rule=rule,\n",
    "            x=x.tolist(),\n",
    "            hidden=x_outs[net.layer1][0].tolist(),\n",
    "            x_root=x_root.tolist(),\n",
    "            hidden_for_x_root=x_root_outs[net.layer1][0].tolist(),\n",
    "            rel_hidden=rel_hidden.tolist(),\n",
    "            hidden_root=hidden_root.tolist(),\n",
    "            logit_x=logit_x.tolist(),\n",
    "            logit_x_root=logit_x_root.tolist(),\n",
    "            hidden_root_for_x_root=hidden_root_for_x_root.tolist(),\n",
    "            rel_hidden_for_x_root=rel_hidden_for_x_root.tolist(),\n",
    "            grad_rel_hidden=grad_rel_hidden.tolist(),\n",
    "            grad_rel_hidden_for_x_root=grad_rel_hidden_for_x_root.tolist(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "df_roots = pd.DataFrame(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_roots[\n",
    "    [\n",
    "        \"rule\",\n",
    "        \"x_root\",\n",
    "        \"hidden\",\n",
    "        \"hidden_for_x_root\",\n",
    "        \"rel_hidden\",\n",
    "        \"rel_hidden_for_x_root\",\n",
    "        \"logit_x\",\n",
    "        \"logit_x_root\",\n",
    "        \"grad_rel_hidden\",\n",
    "        \"grad_rel_hidden_for_x_root\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gamma in [10, 1000, 1_000_000]:\n",
    "    root_gamma = dtd.root_point(\n",
    "        x_outs[net.layer1][0], net.layer2, j=0, rule=\"gamma\", gamma=gamma\n",
    "    )\n",
    "    print(gamma, root_gamma, net.layer2(root_gamma))\n",
    "\n",
    "# rel_gamma = get_relevance_hidden(net, x, rule=\"gamma\", gamma=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roots = []\n",
    "for hidden_idx in range(net.hidden_size):\n",
    "\n",
    "    x_root_search = find_root_point(net, x, hidden_idx, \n",
    "        lambda m, t: get_relevance_hidden(m, t, rule=\"gamma\", gamma=1000),\n",
    "        )\n",
    "    with dtd.record_all_outputs(net) as root_outs:\n",
    "        logits = net(x_root_search.unsqueeze(0))\n",
    "        print(root_outs[net.layer1][0][:, hidden_idx], logits[0])\n",
    "\n",
    "    roots.append(x_root_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_mean = torch.mean(torch.stack(roots), dim=0)\n",
    "\n",
    "with dtd.record_all_outputs(net) as root_outs:\n",
    "    logits = net(root_mean.unsqueeze(0))\n",
    "    print(root_outs[net.layer1][0], logits[0])\n",
    "    print(x_outs[net.layer1][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "\n",
    "net = dtd.NLayerMLP(\n",
    "    n_layers=3,\n",
    "    input_size=2,\n",
    "    hidden_size=30,\n",
    "    output_size=2,\n",
    ")\n",
    "\n",
    "\n",
    "def weight_scale(m: nn.Module) -> nn.Module:\n",
    "    for p in m.parameters():\n",
    "        p.data[p.data > 0] = 1.2 * p.data[p.data > 0]\n",
    "    if isinstance(m, dtd.LinearReLU):\n",
    "        m.linear.bias.data = - m.linear.bias.data.abs() \n",
    "    return m\n",
    "\n",
    "\n",
    "net.apply(weight_scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grid_points = 500\n",
    "\n",
    "torch.manual_seed(0)\n",
    "point = torch.randn(1, net.input_size)\n",
    "\n",
    "grid_line = torch.linspace(-1, 1, n_grid_points)\n",
    "\n",
    "grid = torch.meshgrid(grid_line, grid_line, indexing=\"xy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_a = grid[0].flatten()\n",
    "x_b = grid[1].flatten()\n",
    "\n",
    "inputs = point.repeat(x_a.size(0), 1)\n",
    "inputs[:, 0] = x_a\n",
    "inputs[:, 1] = x_b\n",
    "\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.requires_grad_(True)\n",
    "logits = net(inputs)\n",
    "\n",
    "\n",
    "grads_logit_0, = torch.autograd.grad(\n",
    "    logits[:, 0],\n",
    "    inputs,\n",
    "    grad_outputs=torch.ones_like(logits[:, 0]),\n",
    "    retain_graph=True,\n",
    ")\n",
    "grads_logit_1, = torch.autograd.grad(\n",
    "    logits[:, 1],\n",
    "    inputs,\n",
    "    grad_outputs=torch.ones_like(logits[:, 0]),\n",
    ")\n",
    "\n",
    "for i in range(2):\n",
    "    plt.scatter(\n",
    "        x_a.numpy(),\n",
    "        x_b.numpy(),\n",
    "        # c=net(inputs).argmax(dim=1).numpy(),\n",
    "        # c=net(inputs)[:, 0].detach().numpy(),\n",
    "        c=logits[:, i].detach().numpy(),\n",
    "    )\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Logit {i}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn.apionly as sns\n",
    "import numpy as np\n",
    "colors = np.array(sns.color_palette(\"colorblind\", 100))\n",
    "print(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grad in [grads_logit_0, grads_logit_1]:\n",
    "\n",
    "    grad_colors = dtd.almost_unique(grad, atol=1e-4)\n",
    "    print(grad_colors.unique().shape)\n",
    "    plt.scatter(\n",
    "        x_a.numpy(),\n",
    "        x_b.numpy(),\n",
    "        c=colors[grad_colors.numpy() % len(colors)],\n",
    "        marker='.',\n",
    "        s=1,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (LRP Relations)",
   "language": "python",
   "name": "lrp_py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

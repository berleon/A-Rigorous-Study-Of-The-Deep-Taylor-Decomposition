{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check how the network output changes for the input roots\n",
    "\n",
    "1. Compute the roots of the train-free DTD \n",
    "2. For each input root, we compute the network output \n",
    "3. Is the network output set to zero by the root? No!\n",
    "4. Do the roots have the same gradient as the input? No!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=\"\"\n",
    "\n",
    "\n",
    "from typing import Union, Callable, cast\n",
    "\n",
    "import dataclasses\n",
    "import torch \n",
    "import numpy as np\n",
    "import tqdm.auto\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from lrp_relations import dtd, local_linear\n",
    "from lrp_relations.utils import to_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class NotebookArgs:\n",
    "    root_finder: str = \"linear_dtd\"\n",
    "    explained_output: slice = slice(0, 1)\n",
    "    rule = dtd.rules.z_plus\n",
    "\n",
    "\n",
    "args = NotebookArgs()\n",
    "\n",
    "torch.manual_seed(2)\n",
    "mlp = dtd.MLP(4, 10, 10, 2)\n",
    "mlp.init_weights()\n",
    "\n",
    "print(f\"the network has {sum(p.numel() for p in mlp.parameters())} parameters\")\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "x = mlp.get_input_with_output_greater(\n",
    "    0.25, args.explained_output, non_negative=True\n",
    ")\n",
    "\n",
    "mlp_output = mlp.slice(output=args.explained_output)\n",
    "\n",
    "assert mlp_output(x).shape == (1, 1)\n",
    "\n",
    "\n",
    "x[:, args.explained_output].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.root_finder == \"interpolation\":\n",
    "    root_finder = dtd.InterpolationRootFinder(\n",
    "        mlp,\n",
    "        use_cache=True,\n",
    "        use_candidates_cache=True,\n",
    "        args=local_linear.InterpolationArgs(\n",
    "            batch_size=50,\n",
    "            n_refinement_steps=10,\n",
    "            n_batches=1,\n",
    "            show_progress=True,\n",
    "            enforce_non_negative=True,\n",
    "        ),\n",
    "    )\n",
    "elif args.root_finder == \"linear_dtd\":\n",
    "    root_finder = dtd.LinearDTDRootFinder(\n",
    "        mlp,\n",
    "        args.explained_output.start,\n",
    "        args.rule,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"unknown root_finder: {args.root_finder}\")\n",
    "\n",
    "rel_fn_builder = dtd.TrainFreeFn.get_fn_builder(\n",
    "    mlp,\n",
    "    root_finder=root_finder,\n",
    "    check_consistent=False,\n",
    ")\n",
    "\n",
    "rel_fns = dtd.get_decompose_relevance_fns(\n",
    "    mlp, args.explained_output, rel_fn_builder\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "torch.manual_seed(0)\n",
    "n_errors = 0\n",
    "n_points = 1000\n",
    "pbar = tqdm.auto.tqdm(total=n_points)\n",
    "while True:\n",
    "    try:\n",
    "        x = mlp.get_input_with_output_greater(\n",
    "            0.1,\n",
    "            args.explained_output,\n",
    "            non_negative=True,\n",
    "            seed=int(torch.randint(0, 2**32, (1,)).item()),\n",
    "        )\n",
    "\n",
    "        rel_result = cast(dtd.TrainFreeRel, rel_fns[-1](x))\n",
    "\n",
    "        outputs = torch.cat([mlp_output(r.root) for r in rel_result.roots])\n",
    "\n",
    "        grads = torch.cat(\n",
    "            [mlp_output.compute_input_grad(r.root) for r in rel_result.roots]\n",
    "        )\n",
    "        data.append(\n",
    "            dict(\n",
    "                x=to_np(x),\n",
    "                output_x=mlp_output(x).item(),\n",
    "                grad_input=to_np(mlp_output.compute_input_grad(x)),\n",
    "                output_roots=to_np(outputs),\n",
    "                grad_roots=to_np(grads),\n",
    "            )\n",
    "        )\n",
    "    except AssertionError:\n",
    "        n_errors += 1\n",
    "\n",
    "    pbar.update(1)\n",
    "    pbar.set_postfix(error_percentage=n_errors / pbar.n)\n",
    "    pbar.refresh()\n",
    "    if pbar.n >= n_points:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack(df.output_roots - df.output_x).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = np.stack(df.output_x - df.output_roots)\n",
    "\n",
    "bins = 20\n",
    "plt.hist(diffs.flatten(), bins=bins, density=True)\n",
    "plt.hist(df.output_x, bins=bins, density=True, alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_almost_zero = (diffs < 1e-6).mean()\n",
    "print(\n",
    "    \"Difference between output and roots is almost \"\n",
    "    f\"zero for: {percentage_almost_zero:.2%}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(diffs < 1e-6).mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(\n",
    "    lambda r: np.abs(r.grad_roots - r.grad_input).mean(), axis=1\n",
    ").plot.hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atol = 1e-6\n",
    "grad_diff = df.apply(\n",
    "    lambda r: (np.abs(r.grad_roots - r.grad_input) > atol).any(), axis=1\n",
    ")\n",
    "\n",
    "perc_grad_diff = np.stack(grad_diff).mean()  # type: ignore\n",
    "print(f\"{perc_grad_diff:.2%} of the roots have a gradient difference (>{atol:.1e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

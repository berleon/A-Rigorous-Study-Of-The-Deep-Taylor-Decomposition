{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check how the network output changes for the input roots\n",
    "\n",
    "1. Compute the roots of the train-free DTD \n",
    "2. For each input root, we compute the network output \n",
    "3. Is the network output set to zero by the root? No!\n",
    "4. Do the roots have the same gradient as the input? No!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=\"\"\n",
    "\n",
    "\n",
    "from typing import Union, Callable, cast\n",
    "\n",
    "import dataclasses\n",
    "import torch \n",
    "import numpy as np\n",
    "import tqdm.auto\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from lrp_relations import dtd, local_linear, figures\n",
    "from lrp_relations.utils import to_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class NotebookArgs:\n",
    "    root_finder: str = \"linear_dtd\"\n",
    "    explained_output: slice = slice(0, 1)\n",
    "    rule = dtd.rules.z_plus\n",
    "\n",
    "\n",
    "args = NotebookArgs()\n",
    "\n",
    "torch.manual_seed(1)\n",
    "mlp = dtd.MLP(3, 10, 10, 2)\n",
    "mlp.init_weights()\n",
    "\n",
    "print(f\"the network has {sum(p.numel() for p in mlp.parameters())} parameters\")\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = mlp.get_input_with_output_greater(\n",
    "    0.25, args.explained_output, non_negative=True\n",
    ")\n",
    "\n",
    "mlp_output = mlp.slice(output=args.explained_output)\n",
    "\n",
    "assert mlp_output(x).shape == (1, 1)\n",
    "\n",
    "\n",
    "x[:, args.explained_output].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.root_finder == \"interpolation\":\n",
    "    root_finder = dtd.InterpolationRootFinder(\n",
    "        mlp,\n",
    "        use_cache=True,\n",
    "        use_candidates_cache=True,\n",
    "        args=local_linear.InterpolationArgs(\n",
    "            batch_size=50,\n",
    "            n_refinement_steps=10,\n",
    "            n_batches=1,\n",
    "            show_progress=True,\n",
    "            enforce_non_negative=True,\n",
    "        ),\n",
    "    )\n",
    "elif args.root_finder == \"linear_dtd\":\n",
    "    root_finder = dtd.LinearDTDRootFinder(\n",
    "        mlp,\n",
    "        args.explained_output.start,\n",
    "        args.rule,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"unknown root_finder: {args.root_finder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "torch.manual_seed(0)\n",
    "n_errors = 0\n",
    "n_points = 1000\n",
    "\n",
    "for rule in [\n",
    "    dtd.rules.zero,\n",
    "    dtd.rules.z_plus,\n",
    "    # dtd.GammaRule(0.0),\n",
    "    # dtd.GammaRule(0.25),\n",
    "    # dtd.GammaRule(0.5),\n",
    "    dtd.GammaRule(1.0),\n",
    "    # dtd.GammaRule(100),\n",
    "    dtd.rules.w2,\n",
    "]:\n",
    "    pbar = tqdm.auto.tqdm(total=n_points, desc=rule.name)\n",
    "\n",
    "    root_finder = dtd.LinearDTDRootFinder(\n",
    "        mlp,\n",
    "        args.explained_output.start,\n",
    "        args.rule,\n",
    "    )\n",
    "    rel_fn_builder = dtd.TrainFreeFn.get_fn_builder(\n",
    "        mlp,\n",
    "        root_finder=root_finder,\n",
    "        check_consistent=False,\n",
    "    )\n",
    "\n",
    "    rel_fns = dtd.get_decompose_relevance_fns(\n",
    "        mlp, args.explained_output, rel_fn_builder\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            x = mlp.get_input_with_output_greater(\n",
    "                0.1,\n",
    "                args.explained_output,\n",
    "                non_negative=True,\n",
    "                seed=int(torch.randint(0, 2**32, (1,)).item()),\n",
    "            )\n",
    "\n",
    "            rel_result = cast(dtd.TrainFreeRel, rel_fns[-1](x))\n",
    "\n",
    "            outputs = torch.cat([mlp_output(r.root) for r in rel_result.roots])\n",
    "\n",
    "            grads = torch.cat(\n",
    "                [\n",
    "                    mlp_output.compute_input_grad(r.root)\n",
    "                    for r in rel_result.roots\n",
    "                ]\n",
    "            )\n",
    "            data.append(\n",
    "                dict(\n",
    "                    rule=rule.name,\n",
    "                    gamma=getattr(rule, \"gamma\", \"\"),\n",
    "                    x=to_np(x),\n",
    "                    roots=to_np(torch.cat([r.root for r in rel_result.roots])),\n",
    "                    output_x=mlp_output(x).item(),\n",
    "                    grad_input=to_np(mlp_output.compute_input_grad(x)),\n",
    "                    output_roots=to_np(outputs),\n",
    "                    grad_roots=to_np(grads),\n",
    "                )\n",
    "            )\n",
    "        except AssertionError:\n",
    "            n_errors += 1\n",
    "\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix(error_percentage=n_errors / pbar.n)\n",
    "        pbar.refresh()\n",
    "        if pbar.n >= n_points:\n",
    "            break\n",
    "\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rule.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = np.stack(df.output_x - df.output_roots)\n",
    "\n",
    "bins = 20\n",
    "plt.hist(diffs.flatten(), bins=bins, density=True)\n",
    "plt.hist(df.output_x, bins=bins, density=True, alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_almost_zero = (diffs < 1e-6).mean()\n",
    "print(\n",
    "    \"Difference between output and roots is almost \"\n",
    "    f\"zero for: {percentage_almost_zero:.2%}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(diffs < 1e-6).mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(\n",
    "    lambda r: np.abs(r.grad_roots - r.grad_input).mean(), axis=1\n",
    ").plot.hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atol = 1e-5\n",
    "\n",
    "keys = df.groupby(['rule', 'gamma'], dropna=False).first().index\n",
    "table_data = []\n",
    "for key in keys:\n",
    "    \n",
    "    rule, gamma = key\n",
    "    \n",
    "    print(rule, gamma)\n",
    "    df_sub = df[np.logical_and(df.rule == rule, df.gamma == gamma)]\n",
    "\n",
    "    grad_diff = df_sub.apply(\n",
    "        lambda r: (np.abs(r.grad_roots - r.grad_input) > atol).any(), axis=1\n",
    "    )\n",
    "\n",
    "    perc_grad_diff = np.stack(grad_diff).mean()  # type: ignore\n",
    "    print(key)\n",
    "    print(f\"{perc_grad_diff:.2%} of the roots have a gradient difference (>{atol:.1e})\")\n",
    "\n",
    "    print(f\"{1 - perc_grad_diff:.2%} of the roots have a gradient difference (<{atol:.1e})\")\n",
    "\n",
    "\n",
    "    diffs = np.stack(df_sub.output_roots - df_sub.output_x)\n",
    "\n",
    "    plt.hist(diffs.flatten(), bins=33, density=True)\n",
    "    plt.title(f\"{rule} {gamma}\")\n",
    "    plt.show()\n",
    "    \n",
    "    perc_zero_diff = (np.abs(diffs) < 1e-6).mean()\n",
    "    print(f\"{rule} {gamma}: {perc_zero_diff:.2%} of the roots have a difference of zero\")\n",
    "\n",
    "\n",
    "\n",
    "    if rule == 'gamma':\n",
    "        rule_obj = dtd.GammaRule(gamma)\n",
    "    else:\n",
    "        rule_obj = dtd.Rule(rule)\n",
    "    rule_latex = dtd.get_latex_rule_name(rule_obj)\n",
    "    table_data.append(\n",
    "        {\n",
    "            'Rule': rule_latex,\n",
    "            'In Local Linear Region': f'{perc_grad_diff:.2%}',\n",
    "            'Zero Difference in Output': f'{perc_zero_diff:.2%}',\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(pd.DataFrame(table_data).set_index('Rule').T.to_latex(escape=False).replace('%', '\\%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_diff # .groupby(['rule', 'gamma']).apply(print)\n",
    "df.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_diff = df.apply(\n",
    "    lambda r: (np.abs(r.grad_roots - r.grad_input) <= 1e-8).all(), axis=1\n",
    ")\n",
    "np.stack(grad_diff).mean()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_max = df.apply(\n",
    "    lambda r: (np.abs(r.grad_roots - r.grad_input)).max(), axis=1\n",
    ")\n",
    "\n",
    "diff_max[(diff_max > 1e-8)].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad_diff[(grad_diff < 1e-8)].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_mean = df.apply(\n",
    "    lambda r: (np.abs(r.grad_roots - r.grad_input)).mean(), axis=1\n",
    ")\n",
    "diff_mean[diff_max > 1e-8].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roots_output = []\n",
    "for i in range(len(df)):\n",
    "    roots_output.append(to_np(mlp(torch.from_numpy(df.x.iloc[0]))))\n",
    "\n",
    "df['root_outputs'] = roots_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = df.groupby(['rule', 'gamma'], dropna=False).first().index\n",
    "\n",
    "for key in keys:\n",
    "    rule, gamma = key\n",
    "    print(rule, gamma)\n",
    "    df_sub = df[np.logical_and(df.rule == rule, df.gamma == gamma)]\n",
    "\n",
    "    diffs = np.stack(df_sub.output_roots - df_sub.output_x)\n",
    "\n",
    "    plt.hist(diffs.flatten(), bins=33, density=True)\n",
    "    plt.title(f\"{rule} {gamma}\")\n",
    "    plt.show()\n",
    "    \n",
    "    perc_zero_diff = (np.abs(diffs) < 1e-6).mean()\n",
    "    print(f\"{rule} {gamma}: {perc_zero_diff:.2%} of the roots have a difference of zero\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c_factors = pd.DataFrame(\n",
    "    [dict(t=t, c=to_np(mlp(t*x) / mlp(x))) for t in np.linspace(0, 10, 11)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with figures.latexify():\n",
    "    plt.figure(figsize=figures.get_figure_size(0.3))\n",
    "    for i in range(20):\n",
    "        x = mlp.get_input_with_output_greater(\n",
    "            0.1,\n",
    "            args.explained_output,\n",
    "            non_negative=True,\n",
    "            seed=i,\n",
    "        )\n",
    "\n",
    "        c_factors = pd.DataFrame(\n",
    "            [dict(t=t, c=to_np(mlp(t*x) / mlp(x))) for t in np.linspace(0, 1, 300)]\n",
    "        )\n",
    "        mlp(x)\n",
    "\n",
    "        plt.plot(\n",
    "            c_factors.t,\n",
    "            np.concatenate(c_factors.c)[:, 0],\n",
    "            c='black',\n",
    "            linewidth=0.2,\n",
    "        )\n",
    "        np.stack(c_factors.c).shape\n",
    "\n",
    "    plt.plot(\n",
    "        c_factors.t,\n",
    "        c_factors.t,\n",
    "        linestyle='--',\n",
    "    )\n",
    "# plt.vlines(\n",
    "#     1,\n",
    "#     0,\n",
    "#     plt.ylim()[1],\n",
    "#     linestyle='--',\n",
    "#     color='gray',\n",
    "#     linewidth=0.5,\n",
    "# )\n",
    "plt.xlabel(\"Scaling: c\")\n",
    "plt.ylabel(\"Network Output\")\n",
    "plt.gcf().set_dpi(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = mlp(x, last=mlp.layer1)\n",
    "\n",
    "for c in torch.linspace(0.05, 2, 10):\n",
    "    print((c * a1).shape)\n",
    "    rel = rel_fns[-2](c * a1)\n",
    "    rel.relevance\n",
    "    print(\n",
    "        rel.relevance[:, 6]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(name, param) for name, param in list(mlp.named_parameters())\n",
    " if 'bias' in name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pos_bias = dtd.MLP(3, 10, 10, 2)\n",
    "\n",
    "\n",
    "x = mlp_pos_bias.get_input_with_output_greater(\n",
    "    0.1, \n",
    "    args.explained_output,\n",
    "    n_tries=10000,\n",
    ")\n",
    "[(name, param) for name, param in list(mlp_pos_bias.named_parameters())\n",
    " if 'bias' in name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_finder_bias = dtd.LinearDTDRootFinder(\n",
    "    mlp_pos_bias,\n",
    "    args.explained_output.start,\n",
    "    args.rule,\n",
    ")\n",
    "rel_fn_builder_bias = dtd.TrainFreeFn.get_fn_builder(\n",
    "    mlp_pos_bias,\n",
    "    root_finder=root_finder_bias,\n",
    "    check_consistent=False,\n",
    ")\n",
    "\n",
    "rel_fns_bias = dtd.get_decompose_relevance_fns(\n",
    "    mlp_pos_bias, args.explained_output, rel_fn_builder_bias\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, mlp_pos_bias(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "a1 = mlp_pos_bias(x, last=mlp_pos_bias.layer1)\n",
    "\n",
    "for c in torch.linspace(-20, 20, 10):\n",
    "    print((c * a1).shape)\n",
    "    rel = rel_fns_bias[-2](c * a1)\n",
    "    rel.relevance\n",
    "    print(\n",
    "        rel.relevance[:, 0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "364ea4cd247a9314851a2e26e0d2a5938af3e08d71fd547ec79f80ae6e1a0fa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

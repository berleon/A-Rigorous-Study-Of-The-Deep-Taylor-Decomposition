{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=\"\"\n",
    "\n",
    "import os\n",
    "import socket\n",
    "import pickle\n",
    "from typing import cast\n",
    "\n",
    "import captum.attr\n",
    "import pandas as pd\n",
    "import savethat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    " \n",
    "from lrp_relations import sanity_checks, utils, train_clevr \n",
    "from lrp_relations import data, lrp, gt_eval, figures\n",
    "from relation_network import model as rel_model\n",
    "import savethat.log \n",
    "\n",
    "savethat.log.setup_logger()\n",
    "\n",
    "print(f\"Running on {socket.gethostname()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = utils.get_storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "key = \"SanityChecksForRelationNetworks_2022-06-14T\"\n",
    "runs = pd.DataFrame(storage.find_runs(key))   # type: ignore\n",
    "\n",
    "display(runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = runs.iloc[-1]\n",
    "key = run.run_key\n",
    "print(key)\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(storage / key / \"results.pickle\", 'rb') as f:\n",
    "    result = cast(sanity_checks.SanityChecksForRelationNetworksResults,\n",
    "         pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.saliency_0.image_idx, result.saliency_0.question_index\n",
    "result.saliency_0_rand_questions.image_idx, result.saliency_0_rand_questions.question_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = sanity_checks.SanityChecksForRelationNetworksArgs.from_json(\n",
    "    storage / key / \"args.json\"\n",
    ")\n",
    "\n",
    "dataset = data.CLEVR_XAI(\n",
    "    question_type=args.question_type,\n",
    "    ground_truth=args.ground_truth,\n",
    "    reverse_question=True,\n",
    "    use_preprocessed=False,\n",
    ")\n",
    "\n",
    "display(dataset.get_image(0, preprocessed=False, resize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.answer_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 2\n",
    "ncols = 8\n",
    "# for saliency_result in [\n",
    "#     result.saliency_0,\n",
    "#     result.saliency_1,\n",
    "#     result.saliency_0_rand_questions\n",
    "# ]:\n",
    "\n",
    "answer_dict = dataset.answer_dict()\n",
    "with figures.latexify():\n",
    "    figsize = figures.get_figure_size(fraction=1.0, ratio=0.38)\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "\n",
    "    for i, (ax1, ax2, ax3, ax4) in enumerate(\n",
    "        zip(\n",
    "            axes.flatten()[::4],\n",
    "            axes.flatten()[1::4],\n",
    "            axes.flatten()[2::4],\n",
    "            axes.flatten()[3::4],\n",
    "        )\n",
    "    ):\n",
    "\n",
    "        saliency_result = result.saliency_0\n",
    "        image_idx: int = int(saliency_result.image_idx[i])\n",
    "        question_index: int = int(saliency_result.question_index[i])\n",
    "\n",
    "        quest, answer0 = dataset.get_question_and_answer(question_index)\n",
    "        answer1 = dataset.get_question_and_answer(\n",
    "            int(result.saliency_1.question_index[i])\n",
    "        )\n",
    "        img = dataset.get_image(image_idx, preprocessed=False, resize=True)\n",
    "        ax1.imshow(img)\n",
    "        ax2.set_title(utils.insert_newlines(quest, every=40), fontsize=6)\n",
    "\n",
    "        saliency = lrp.normalize_saliency(saliency_result.saliency[i])\n",
    "        im = ax2.imshow(saliency.mean(0), cmap=\"Reds\")\n",
    "\n",
    "        ax3.imshow(\n",
    "            lrp.normalize_saliency(result.saliency_1.saliency[i]).mean(0),\n",
    "            cmap=\"Reds\",\n",
    "        )\n",
    "        ax4.imshow(\n",
    "            lrp.normalize_saliency(\n",
    "                result.saliency_0_rand_questions.saliency[i]\n",
    "            ).mean(0),\n",
    "            cmap=\"Reds\",\n",
    "        )\n",
    "\n",
    "        rand_q_index = int(result.saliency_0_rand_questions.question_index[i])\n",
    "        rand_quest, answer_rand = dataset.get_question_and_answer(rand_q_index)\n",
    "        ax4.set_title(utils.insert_newlines(rand_quest, every=16), fontsize=6)\n",
    "        # plt.colorbar(im, ax=ax2)\n",
    "\n",
    "        ax1.set_xlabel(\"Input\")\n",
    "\n",
    "        for ax, sal_res in [\n",
    "            (ax2, saliency_result),\n",
    "            (ax3, result.saliency_1),\n",
    "            (ax4, result.saliency_0_rand_questions),\n",
    "        ]:\n",
    "            answer = answer_dict[int(sal_res.target[i].item())]\n",
    "            ax.set_xlabel(f\"{answer}\", fontsize=8, fontname=\"monospace\")\n",
    "\n",
    "    for ax in axes.flatten():\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    fig.set_dpi(120)\n",
    "    fig.subplots_adjust(wspace=0.15, hspace=0.75, left=0.10, right=0.90)\n",
    "    fig_path = storage / key / \"saliency\" / \"saliency.pgf\"\n",
    "    fig_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"scp -r {socket.gethostname()}:{fig_path.parent} ./figures\")\n",
    "    figures.savefig_pgf(fig, fig_path, pad_inches=0.15)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = result.statistics(\n",
    "    lambda x: lrp.normalize_saliency(\n",
    "        x, clip_percentile_min=0.5, clip_percentile_max=99.5\n",
    "    )\n",
    ")\n",
    "display(df)\n",
    "\n",
    "print(df.abs_mean.iloc[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert args.model is not None\n",
    "\n",
    "with open(storage / args.model / \"results.pickle\", \"rb\") as f:\n",
    "    model_ckpts = cast(train_clevr.TrainedModel, pickle.load(f))\n",
    "\n",
    "model_args = train_clevr.TrainArgs.from_json(storage / args.model / \"args.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = model_ckpts.get_checkpoint(args.checkpoint).accuracy\n",
    "print(f\"Model accuracy [%]: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (LRP Relations)",
   "language": "python",
   "name": "lrp_py39"
  },
  "vscode": {
   "interpreter": {
    "hash": "359b1ad37b7141b7748efb8a3badd050b0020e392ef93fbacf6b31a953b41e53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
